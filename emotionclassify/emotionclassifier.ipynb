{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79eecd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40004e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size=100\n",
    "embed_size=300\n",
    "learningrate=0.0001\n",
    "emotionlist=['sadness','joy','love','anger','fear','surprise']\n",
    "emotiondict={emotionlist[i]:i for i in range(len(emotionlist))}\n",
    "def idx2emotion(idx):\n",
    "    if not isinstance(idx,(tuple,list)):\n",
    "        return emotionlist[idx]\n",
    "    return [idx2emotion(i) for i in idx]\n",
    "def emotion2idx(emotion):\n",
    "    if not isinstance(emotion,(tuple,list)):\n",
    "        return emotiondict[emotion]\n",
    "    return [emotion2idx(i) for i in emotion]\n",
    "print(idx2emotion([1,4,3,2]))\n",
    "print(emotion2idx(['sadness','joy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02d484c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"merged_training.pkl\")\n",
    "df=df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e6722e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.text.head())\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6f9642",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab():\n",
    "    def __init__(self,tokens,min_freq=0):\n",
    "        self.token_freq = {}\n",
    "        for sentence in tokens:\n",
    "            for token in sentence.split():\n",
    "                if token not in self.token_freq:\n",
    "                    self.token_freq[token] = 1\n",
    "                else:\n",
    "                    self.token_freq[token] += 1\n",
    "        self.itos = [\"<pad>\",\"<unk>\"]\n",
    "        for token,freq in self.token_freq.items():\n",
    "            if freq >= min_freq:\n",
    "                self.itos.append(token)\n",
    "        self.itos[2:]=sorted(self.itos[2:],key=lambda x:self.token_freq[x],reverse=True)\n",
    "        self.stoi = {token:idx for idx,token in enumerate(self.itos)}\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "    def __getitem__(self,idx):\n",
    "        if not isinstance(idx,(list,tuple)):\n",
    "            return self.itos[idx] if idx!=0 else ''\n",
    "        return [self.__getitem__(i) for i in idx]\n",
    "    def tokens2idx(self,tokens):\n",
    "        if not isinstance(tokens,(list,tuple)):\n",
    "            if ' ' in tokens:\n",
    "                return self.tokens2idx(tokens.split())\n",
    "            if tokens in self.stoi:\n",
    "                return self.stoi[tokens]\n",
    "            return self.stoi['<unk>']\n",
    "        return [self.tokens2idx(i) for i in tokens]\n",
    "vocab=Vocab(df.text,min_freq=10)\n",
    "print(vocab[(0,1,2,3,4,5,6,7,8,9)])\n",
    "print(vocab.stoi['i'],vocab.tokens2idx(['i','feel']))\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0fd5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfanger=df[df.emotions=='anger']\n",
    "print(dfanger.head())\n",
    "print(df.head())\n",
    "df2=pd.concat([dfanger,df],ignore_index=True)\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662fd613",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(data.Dataset):\n",
    "    def __init__(self,datasource,vocab):\n",
    "        self.set=datasource.reset_index(drop=True)\n",
    "        self.vocab=vocab\n",
    "    def __getitem__(self, index):\n",
    "        c=vocab.tokens2idx(self.set.text[index])\n",
    "        if not isinstance(c,list):\n",
    "            c=[c]\n",
    "        if len(c)>=50:\n",
    "            c=c[:50]\n",
    "        else:\n",
    "            c=c+[0]*(50-len(c))\n",
    "        return (torch.tensor(c),self.set.emotions[index])\n",
    "    def __len__(self):\n",
    "        return len(self.set)\n",
    "trainset=dataset(df[:350000],vocab)\n",
    "testset=dataset(df[350000:],vocab)\n",
    "load_trainset=data.DataLoader(trainset,batch_size=batch_size,shuffle=True,drop_last=True)\n",
    "load_testset=data.DataLoader(testset,batch_size=batch_size,shuffle=True,drop_last=True)\n",
    "print(trainset[3])\n",
    "print(vocab[list(trainset[3][0])])\n",
    "print(df.text[3],df.emotions[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a311771",
   "metadata": {},
   "outputs": [],
   "source": [
    "class emotion(nn.Module):\n",
    "    def __init__(self,vocab_size,embed_size,num_classes,dropout=0.5):\n",
    "        super(emotion,self).__init__()\n",
    "        self.embedding=nn.Embedding(vocab_size,embed_size)\n",
    "        self.multiheadattention1=nn.TransformerEncoderLayer(embed_size,5,batch_first=True,dropout=dropout)\n",
    "        self.multiheadattention2=nn.TransformerEncoderLayer(embed_size,5,batch_first=True,dropout=dropout)\n",
    "        self.multiheadattention3=nn.TransformerEncoderLayer(embed_size,5,batch_first=True,dropout=dropout)\n",
    "        self.fc=nn.Linear(embed_size,num_classes)\n",
    "        self.softmax=nn.Softmax()\n",
    "    def forward(self,x):\n",
    "        mask=~(x==0)\n",
    "        # print(mask)\n",
    "        x=self.embedding(x)\n",
    "        x=self.multiheadattention1(x,src_key_padding_mask=mask)\n",
    "        x=self.multiheadattention2(x,src_key_padding_mask=mask)\n",
    "        x=self.multiheadattention3(x,src_key_padding_mask=mask)\n",
    "        # print(x.shape)\n",
    "        # print(mask.shape)\n",
    "        # print(torch.mean(x,dim=1).shape)\n",
    "        # print(torch.sum(mask,dim=1))\n",
    "        x=torch.sum(x,dim=1)/torch.sum(mask,dim=1).unsqueeze(dim=1)\n",
    "        #avg pool->attention pool\n",
    "        x=self.fc(x)\n",
    "        x=self.softmax(x)\n",
    "        return x\n",
    "model=emotion(len(vocab),embed_size,len(emotionlist))\n",
    "model.to(device=device)\n",
    "print(model(torch.ones(3,5,device=device).long()))\n",
    "print(model(torch.tensor([[1,1,1,0,0],[3,6,5,3,0],[3,8,6,5,0]],device=device).long()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2761c429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict1sentence(model,sentence,vocab):\n",
    "    model.eval()\n",
    "    c=vocab.tokens2idx(sentence.split())\n",
    "    if not isinstance(c,list):\n",
    "        c=[c]\n",
    "    if len(c)>=50:\n",
    "        c=c[:50]\n",
    "    else:\n",
    "        c=c+[0]*(50-len(c))\n",
    "    c=torch.tensor(c)\n",
    "    c=c.to(device=device)\n",
    "    prediction=model(c.unsqueeze(dim=0))\n",
    "    # print(c)\n",
    "    # print(prediction)\n",
    "    # print(torch.argmax(prediction,dim=1))\n",
    "    return idx2emotion(torch.argmax(prediction,dim=1))\n",
    "print(predict1sentence(model,'i take every day as it comes i m just focussing on eating better at the moment i m not aiming for unrealistic targets or setting myself deadlines because then i feel pressured i m just improving my diet',vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899a29ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(model,load_testset):\n",
    "    correct=0\n",
    "    total=0\n",
    "    countclass=[0 for i in range(6)]\n",
    "\n",
    "    for datapair in load_testset:\n",
    "        text,label=datapair\n",
    "        text=text.to(device)\n",
    "        label=torch.tensor(emotion2idx(label))\n",
    "        label=label.to(device)\n",
    "        answer=model(text)\n",
    "        answer=torch.argmax(answer,dim=1)\n",
    "        for i in range(6):\n",
    "            countclass[i]+=torch.count_nonzero(answer==torch.tensor([i],device=device))\n",
    "        correct+=torch.count_nonzero(answer==label)\n",
    "        total+=batch_size\n",
    "    return correct,correct/total,countclass\n",
    "print(get_acc(model,load_testset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7acf29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=learningrate)\n",
    "loss=nn.CrossEntropyLoss()\n",
    "loss.to(device=device)\n",
    "for epoch in range(5):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        acc=get_acc(model,load_testset=load_testset)\n",
    "        print(acc)\n",
    "        acc=get_acc(model,load_trainset)\n",
    "        print(acc)\n",
    "    model.train()\n",
    "    totalloss=0\n",
    "    for number,datapair in enumerate(load_trainset):\n",
    "        # print(number)\n",
    "        text,label=datapair\n",
    "        text=text.to(device)\n",
    "        label=torch.tensor(emotion2idx(label))\n",
    "        # print(label)\n",
    "        label=label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        result=model(text)\n",
    "        # print(result)\n",
    "        trainloss=loss(result,label)\n",
    "        trainloss.backward()\n",
    "        optimizer.step()\n",
    "        totalloss+=trainloss\n",
    "        if number%1000==0:\n",
    "            print(number)\n",
    "            print(totalloss/number)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
