{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79eecd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40004e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "max_len=50\n",
    "min_freq=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a3a6b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['joy', 'fear', 'anger', 'love']\n",
      "[0, 1]\n"
     ]
    }
   ],
   "source": [
    "emotionlist=['sadness','joy','love','anger','fear','surprise']\n",
    "emotiondict={emotionlist[i]:i for i in range(len(emotionlist))}\n",
    "def idx2emotion(idx):\n",
    "    if not isinstance(idx,(tuple,list)):\n",
    "        return emotionlist[idx]\n",
    "    return [idx2emotion(i) for i in idx]\n",
    "def emotion2idx(emotion):\n",
    "    if not isinstance(emotion,(tuple,list)):\n",
    "        return emotiondict[emotion]\n",
    "    return [emotion2idx(i) for i in emotion]\n",
    "print(idx2emotion([1,4,3,2]))\n",
    "print(emotion2idx(['sadness','joy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a02d484c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "416809\n",
      "sadness:0.2907494799776396\n",
      "joy:0.3384451871240784\n",
      "love:0.0829012809224369\n",
      "anger:0.13751382527728528\n",
      "fear:0.11446969715145307\n",
      "surprise:0.03592052954710671\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_pickle(\"merged_training.pkl\")\n",
    "df=df.reset_index(drop=True)\n",
    "print(len(df))\n",
    "for i in emotionlist:\n",
    "    print(f'{i}:{len(df[df.emotions==i])/len(df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b6f9642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '<unk>', 'i', 'feel', 'and', 'to', 'the', 'a', 'feeling', 'that']\n",
      "2 [2, 3]\n",
      "15334\n"
     ]
    }
   ],
   "source": [
    "class Vocab():\n",
    "    def __init__(self,tokens,min_freq=0):\n",
    "        self.token_freq = {}\n",
    "        for sentence in tokens:\n",
    "            for token in sentence.split():\n",
    "                if token not in self.token_freq:\n",
    "                    self.token_freq[token] = 1\n",
    "                else:\n",
    "                    self.token_freq[token] += 1\n",
    "        self.itos = [\"<pad>\",\"<unk>\"]\n",
    "        for token,freq in self.token_freq.items():\n",
    "            if freq >= min_freq:\n",
    "                self.itos.append(token)\n",
    "        self.itos[2:]=sorted(self.itos[2:],key=lambda x:self.token_freq[x],reverse=True)\n",
    "        self.stoi = {token:idx for idx,token in enumerate(self.itos)}\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "    def __getitem__(self,idx):\n",
    "        if not isinstance(idx,(list,tuple)):\n",
    "            return self.itos[idx] if idx!=0 else ''\n",
    "        return [self.__getitem__(i) for i in idx]\n",
    "    def tokens2idx(self,tokens):\n",
    "        if not isinstance(tokens,(list,tuple)):\n",
    "            if ' ' in tokens:\n",
    "                return self.tokens2idx(tokens.split())\n",
    "            if tokens in self.stoi:\n",
    "                return self.stoi[tokens]\n",
    "            return self.stoi['<unk>']\n",
    "        return [self.tokens2idx(i) for i in tokens]\n",
    "vocab=Vocab(df.text,min_freq=min_freq)\n",
    "print(vocab[(0,1,2,3,4,5,6,7,8,9)])\n",
    "print(vocab.stoi['i'],vocab.tokens2idx(['i','feel']))\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac0fd5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     text emotions\n",
      "0       i feel awful about it too because it s my job ...  sadness\n",
      "1                                   im alone i feel awful  sadness\n",
      "2                i was feeling a little low few days back  sadness\n",
      "3       i also feel disillusioned that someone who cla...  sadness\n",
      "4       i wish you knew every word i write i write for...  sadness\n",
      "...                                                   ...      ...\n",
      "556191  i just feel like screaming at myself for being...     love\n",
      "556192  i always end up watching one of his dvd s as i...      joy\n",
      "556193  i can t seem to describe how i am feeling in a...      joy\n",
      "556194  i will never let you feel unloved for a second...  sadness\n",
      "556195  i could sense that he was uncomfortable when h...     fear\n",
      "\n",
      "[556196 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df1=df[:350000]\n",
    "dfsad=df1[df1.emotions=='sadness']\n",
    "dfanger=df1[df1.emotions=='anger']\n",
    "dffear=df1[df1.emotions=='fear']\n",
    "dflove=df1[df1.emotions=='love']\n",
    "dfsurprise=df1[df1.emotions=='surprise']\n",
    "df2=pd.concat([dfsad[:7000],dfanger,dfanger[:16000],dffear,dffear[:27000],dflove,dflove,dflove[:10000],df1],ignore_index=True)\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "662fd613",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(data.Dataset):\n",
    "    def __init__(self,datasource,vocab):\n",
    "        self.set=datasource.reset_index(drop=True)\n",
    "        self.vocab=vocab\n",
    "    def __getitem__(self, index):\n",
    "        c=vocab.tokens2idx(self.set.text[index])\n",
    "        if not isinstance(c,list):\n",
    "            c=[c]\n",
    "        if len(c)>=max_len:\n",
    "            c=c[:max_len]\n",
    "        else:\n",
    "            c=c+[0]*(max_len-len(c))\n",
    "        return (torch.tensor(c),self.set.emotions[index])\n",
    "    def __len__(self):\n",
    "        return len(self.set)\n",
    "trainset=dataset(df2,vocab)\n",
    "testset=dataset(df[350000:],vocab)\n",
    "# print(trainset[3])\n",
    "# print(vocab[list(trainset[3][0])])\n",
    "# print(df.text[3],df.emotions[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ecbffb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 位置编码\n",
    "class PositionalEncoding(nn.Module):\n",
    "    # 初始化函数，接收隐藏单元数量、dropout率和最大序列长度作为输入\n",
    "    def __init__(self, num_hiddens, max_len=1000):\n",
    "        # 调用父类的初始化函数\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # 创建一个dropout层，用于随机丢弃输入的元素\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # 创建一个形状为(1, max_len, num_hiddens)的位置编码张量P，初始化为全0\n",
    "        self.P = torch.zeros((1, max_len, num_hiddens))\n",
    "        # 生成位置编码矩阵X，其中每一行表示一个位置的编码，编码方式采用sin和cos函数\n",
    "        # 编码公式：X[i, j] = sin(i / 10000^(2j / num_hiddens)) 或 cos(i / 10000^(2j / num_hiddens))\n",
    "        X = torch.arange(max_len, dtype=torch.float32).reshape(\n",
    "            -1, 1) / torch.pow(10000, \n",
    "                               torch.arange(0, num_hiddens, 2, dtype=torch.float32) /\n",
    "                               num_hiddens)\n",
    "        # 将位置编码矩阵中的偶数维度的元素替换为sin函数的结果\n",
    "        self.P[:,:,0::2] = torch.sin(X)\n",
    "        # 将位置编码矩阵中的奇数维度的元素替换为cos函数的结果\n",
    "        self.P[:,:,1::2] = torch.cos(X)\n",
    "    # 前向传播函数，接收输入张量X作为输入\n",
    "    def forward(self, X):\n",
    "        # 将位置编码张量P与输入张量X相加，并将结果移动到与X相同的设备上\n",
    "        X = X + self.P[:, :X.shape[1], :].to(X.device)\n",
    "        # 对相加后的结果应用dropout，并返回结果\n",
    "        return self.dropout(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a311771",
   "metadata": {},
   "outputs": [],
   "source": [
    "class emotion(nn.Module):\n",
    "    def __init__(self,vocab_size,embed_size,num_classes,dropout=0.5):\n",
    "        super(emotion,self).__init__()\n",
    "        self.embedding=nn.Embedding(vocab_size,embed_size)\n",
    "        # self.pos=PositionalEncoding(embed_size,max_len=max_len)\n",
    "        self.multiheadattention1=nn.TransformerEncoderLayer(embed_size,5,batch_first=True,dropout=dropout)\n",
    "        self.multiheadattention2=nn.TransformerEncoderLayer(embed_size,5,batch_first=True,dropout=dropout)\n",
    "        self.multiheadattention3=nn.TransformerEncoderLayer(embed_size,5,batch_first=True,dropout=dropout)\n",
    "        self.fc=nn.Linear(embed_size,num_classes)\n",
    "        #self.softmax=nn.Softmax()\n",
    "    def forward(self,x):\n",
    "        mask=~(x==0)\n",
    "        x=self.embedding(x)\n",
    "        # x=self.pos(x)\n",
    "        x=self.multiheadattention1(x,src_key_padding_mask=mask)\n",
    "        x=self.multiheadattention2(x,src_key_padding_mask=mask)\n",
    "        x=self.multiheadattention3(x,src_key_padding_mask=mask)\n",
    "        x=torch.sum(x,dim=1)/torch.sum(mask,dim=1).unsqueeze(dim=1)\n",
    "        x=self.fc(x)\n",
    "        #x=self.softmax(x)\n",
    "        return x\n",
    "# model=emotion(len(vocab),embed_size,len(emotionlist),dropout=dropout)\n",
    "# model.to(device=device)\n",
    "# a=torch.ones(1,5,device=device).long()\n",
    "# print(model(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2761c429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict1sentence(model,sentence,vocab):\n",
    "    model.eval()\n",
    "    c=vocab.tokens2idx(sentence.lower().split())\n",
    "    if not isinstance(c,list):\n",
    "        c=[c]\n",
    "    if len(c)>=max_len:\n",
    "        c=c[:max_len]\n",
    "    else:\n",
    "        c=c+[0]*(max_len-len(c))\n",
    "    c=torch.tensor(c)\n",
    "    c=c.to(device=device)\n",
    "    # print(c.unsqueeze(dim=0).shape)\n",
    "    prediction=model(c.unsqueeze(dim=0))\n",
    "    # print(c)\n",
    "    # print(prediction)\n",
    "    # print(torch.argmax(prediction,dim=1))\n",
    "    return idx2emotion(torch.argmax(prediction,dim=1))\n",
    "# print(predict1sentence(model,'For all the hours I spent in the gym working and training and Vanessa you holding down the family the way that you have I cant theres no way that I can thank you enough so from the bottom of my heart thank you And what can I say Mamba out',vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "899a29ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(model,load_testset,batch_size):\n",
    "    model.eval()\n",
    "    correct=0\n",
    "    total=0\n",
    "    countclass=torch.zeros(6)\n",
    "    totalclass=torch.zeros(6)\n",
    "    for datapair in load_testset:\n",
    "        text,label=datapair\n",
    "        text=text.to(device)\n",
    "        label=torch.tensor(emotion2idx(label))\n",
    "        label=label.to(device)\n",
    "        answer=model(text)\n",
    "        answer=torch.argmax(answer,dim=1)\n",
    "        for i in range(6):\n",
    "            countclass[i]+=torch.count_nonzero(answer==torch.tensor([i],device=device)).item()\n",
    "            totalclass[i]+=torch.count_nonzero(label==torch.tensor([i],device=device)).item()\n",
    "        correct+=torch.count_nonzero(answer==label)\n",
    "        total+=batch_size\n",
    "        \n",
    "    return correct,correct/total,countclass,totalclass,countclass/totalclass\n",
    "# print(get_acc(model,load_testset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0926796e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-24 08:22:19,270] A new study created in memory with name: no-name-5af64d6e-4e51-4db8-a892-650090564846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(17763, device='cuda:0'), tensor(0.2660, device='cuda:0'), tensor([6.1036e+04, 0.0000e+00, 4.2000e+01, 2.0000e+01, 4.2000e+01, 5.6440e+03]), tensor([19272., 22763.,  5533.,  9162.,  7702.,  2352.]), tensor([3.1671e+00, 0.0000e+00, 7.5908e-03, 2.1829e-03, 5.4531e-03, 2.3997e+00]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-10-24 08:23:22,525] Trial 0 failed with parameters: {'batch_size': 32, 'embed_size': 100, 'learningrate': 0.0009102701171420899, 'dropout': 0.25367775414806226, 'optimizer': 'SGD'} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\pt\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\22846\\AppData\\Local\\Temp\\ipykernel_13220\\3841559485.py\", line 18, in objective\n",
      "    acc=get_acc(model,load_trainset,batch_size=batch_size)\n",
      "  File \"C:\\Users\\22846\\AppData\\Local\\Temp\\ipykernel_13220\\1378330542.py\", line 12, in get_acc\n",
      "    answer=model(text)\n",
      "  File \"d:\\Anaconda\\envs\\pt\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\pt\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"C:\\Users\\22846\\AppData\\Local\\Temp\\ipykernel_13220\\3712188409.py\", line 16, in forward\n",
      "    x=self.multiheadattention2(x,src_key_padding_mask=mask)\n",
      "  File \"d:\\Anaconda\\envs\\pt\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\pt\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\pt\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 937, in forward\n",
      "    x = self.norm2(x + self._ff_block(x))\n",
      "KeyboardInterrupt\n",
      "[W 2025-10-24 08:23:22,536] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 51\u001b[0m\n\u001b[0;32m     49\u001b[0m         acc\u001b[38;5;241m=\u001b[39mget_acc(model,load_testset,batch_size)\n\u001b[0;32m     50\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m acc[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m---> 51\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m trail\u001b[38;5;241m=\u001b[39mstudy\u001b[38;5;241m.\u001b[39mbest_trial\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrail\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pt\\lib\\site-packages\\optuna\\study\\study.py:490\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    390\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    397\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    398\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    399\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    400\u001b[0m \n\u001b[0;32m    401\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    489\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 490\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pt\\lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pt\\lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial_id \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pt\\lib\\site-packages\\optuna\\study\\_optimize.py:258\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    254\u001b[0m     updated_state \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    257\u001b[0m ):\n\u001b[1;32m--> 258\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trial\u001b[38;5;241m.\u001b[39m_trial_id\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pt\\lib\\site-packages\\optuna\\study\\_optimize.py:201\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 201\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    204\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[13], line 18\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     16\u001b[0m acc\u001b[38;5;241m=\u001b[39mget_acc(model,load_testset\u001b[38;5;241m=\u001b[39mload_testset,batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(acc)\n\u001b[1;32m---> 18\u001b[0m acc\u001b[38;5;241m=\u001b[39m\u001b[43mget_acc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mload_trainset\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(acc)\n\u001b[0;32m     20\u001b[0m freq\u001b[38;5;241m=\u001b[39macc[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "Cell \u001b[1;32mIn[12], line 12\u001b[0m, in \u001b[0;36mget_acc\u001b[1;34m(model, load_testset, batch_size)\u001b[0m\n\u001b[0;32m     10\u001b[0m label\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor(emotion2idx(label))\n\u001b[0;32m     11\u001b[0m label\u001b[38;5;241m=\u001b[39mlabel\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 12\u001b[0m answer\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m answer\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39margmax(answer,dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m6\u001b[39m):\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pt\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pt\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[10], line 16\u001b[0m, in \u001b[0;36memotion.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# x=self.pos(x)\u001b[39;00m\n\u001b[0;32m     15\u001b[0m x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultiheadattention1(x,src_key_padding_mask\u001b[38;5;241m=\u001b[39mmask)\n\u001b[1;32m---> 16\u001b[0m x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultiheadattention2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultiheadattention3(x,src_key_padding_mask\u001b[38;5;241m=\u001b[39mmask)\n\u001b[0;32m     18\u001b[0m x\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39msum(x,dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m/\u001b[39mtorch\u001b[38;5;241m.\u001b[39msum(mask,dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pt\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pt\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pt\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:937\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[1;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    933\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(\n\u001b[0;32m    934\u001b[0m         x\n\u001b[0;32m    935\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sa_block(x, src_mask, src_key_padding_mask, is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[0;32m    936\u001b[0m     )\n\u001b[1;32m--> 937\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ff_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    939\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "study=optuna.create_study(direction=\"maximize\")\n",
    "def objective(trial):\n",
    "    batch_size=trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "    embed_size=trial.suggest_categorical(\"embed_size\", [50, 100, 200])\n",
    "    learningrate=trial.suggest_float(\"learningrate\", 1e-5, 1e-2, log=True)\n",
    "    dropout=trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    optimize=trial.suggest_categorical(\"optimizer\",['Adam','SGD'])\n",
    "    model=emotion(len(vocab),embed_size=embed_size,num_classes=len(emotionlist),dropout=dropout)\n",
    "    model.to(device)\n",
    "    optimizer=getattr(torch.optim,optimize)(model.parameters(),lr=learningrate)\n",
    "    load_trainset=data.DataLoader(trainset,batch_size=batch_size,shuffle=True,drop_last=True)\n",
    "    load_testset=data.DataLoader(testset,batch_size=batch_size,shuffle=True,drop_last=True)\n",
    "    for epoch in range(3):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            acc=get_acc(model,load_testset=load_testset,batch_size=batch_size)\n",
    "            print(acc)\n",
    "            acc=get_acc(model,load_trainset,batch_size=batch_size)\n",
    "            print(acc)\n",
    "            freq=acc[-1]\n",
    "            weight=nn.functional.softmax(-freq)\n",
    "        loss=nn.CrossEntropyLoss(weight=weight*6)\n",
    "        loss.to(device=device)\n",
    "        model.train()\n",
    "        # totalloss=0\n",
    "        for number,datapair in enumerate(load_trainset):\n",
    "            # print(number)\n",
    "            text,label=datapair\n",
    "            text=text.to(device)\n",
    "            label=torch.tensor(emotion2idx(label))\n",
    "            # print(label)\n",
    "            label=label.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            result=model(text)\n",
    "            # print(result)\n",
    "            trainloss=loss(result,label)\n",
    "            trainloss.backward()\n",
    "            optimizer.step()\n",
    "            # totalloss+=trainloss\n",
    "            # if number==100:\n",
    "            #     break\n",
    "            #     print(number)\n",
    "            #     print(totalloss/number)\n",
    "        acc=get_acc(model,load_testset,batch_size)\n",
    "        trial.report(acc[1],epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "    with torch.no_grad():\n",
    "        acc=get_acc(model,load_testset,batch_size)\n",
    "        return acc[1]\n",
    "study.optimize(objective, n_trials=5)\n",
    "trail=study.best_trial\n",
    "print(f'value:{trail.value}')\n",
    "print(f'params:{trail.params.items()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954e700b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size=200\n",
    "dropout=0.3\n",
    "learningrate=0.0075\n",
    "batch_size=128\n",
    "model=emotion(len(vocab),embed_size,len(emotionlist),dropout=dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7acf29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(60279, device='cuda:0'), tensor(0.9024, device='cuda:0'), tensor([18438., 22071.,  6945.,  9840.,  7099.,  2407.]), tensor([19277., 22765.,  5535.,  9167.,  7701.,  2355.]), tensor([0.9565, 0.9695, 1.2547, 1.0734, 0.9218, 1.0221]))\n",
      "(tensor(501208, device='cuda:0'), tensor(0.9013, device='cuda:0'), tensor([108548., 121418., 100637., 115961.,  93027.,  16509.]), tensor([108886., 118279.,  97036., 112280., 107004.,  12615.]), tensor([0.9969, 1.0265, 1.0371, 1.0328, 0.8694, 1.3087]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\22846\\AppData\\Local\\Temp\\ipykernel_54912\\2797634214.py:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  weight=nn.functional.softmax(-freq)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor(inf, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "1000\n",
      "tensor(0.2628, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "2000\n",
      "tensor(0.2623, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "3000\n",
      "tensor(0.2612, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "4000\n",
      "tensor(0.2600, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "5000\n",
      "tensor(0.2588, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=learningrate)\n",
    "load_trainset=data.DataLoader(trainset,batch_size=batch_size,shuffle=True,drop_last=True)\n",
    "load_testset=data.DataLoader(testset,batch_size=batch_size,shuffle=True,drop_last=True)\n",
    "for epoch in range(1):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        acc=get_acc(model,load_testset=load_testset,batch_size=batch_size)\n",
    "        print(acc)\n",
    "        acc=get_acc(model,load_trainset,batch_size=batch_size)\n",
    "        print(acc)\n",
    "        freq=acc[-1]\n",
    "        weight=nn.functional.softmax(-freq)\n",
    "    loss=nn.CrossEntropyLoss(weight=weight*6)\n",
    "    loss.to(device=device)\n",
    "    model.train()\n",
    "    totalloss=0\n",
    "    for number,datapair in enumerate(load_trainset):\n",
    "        # print(number)\n",
    "        text,label=datapair\n",
    "        text=text.to(device)\n",
    "        label=torch.tensor(emotion2idx(label))\n",
    "        # print(label)\n",
    "        label=label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        result=model(text)\n",
    "        # print(result)\n",
    "        trainloss=loss(result,label)\n",
    "        trainloss.backward()\n",
    "        optimizer.step()\n",
    "        totalloss+=trainloss\n",
    "        if number%1000==0:\n",
    "            print(number)\n",
    "            print(totalloss/number)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "a578c845",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,'6classes,emotion,withposinfo,acc=9024')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342d8281",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=torch.load('5classes,emotion',weights_only=False)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3573df34",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "import random\n",
    "start=random.randint(1,50000)\n",
    "for i in range(start,start+20):\n",
    "    p=testset[i][0]\n",
    "    p=p.to(device=device)\n",
    "    predict=model(p.unsqueeze(dim=0))\n",
    "    predictemotion=idx2emotion(torch.argmax(predict,dim=1))\n",
    "    sentence=vocab[testset[i][0].tolist()]\n",
    "    try:\n",
    "        sentence.remove('')\n",
    "    except:\n",
    "        pass\n",
    "    sentence=' '.join(sentence)\n",
    "    print(f'原句：{sentence}')\n",
    "    print(f'预测感情：{predictemotion},实际感情：{testset[i][1]}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
